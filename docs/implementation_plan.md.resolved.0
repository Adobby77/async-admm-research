# Implementation Plan - ADMM for Lasso

ADMM (Alternating Direction Method of Multipliers) implementation for the Lasso problem:
$$ \min_x \frac{1}{2} \|Ax - b\|_2^2 + \lambda \|x\|_1 $$

## User Review Required
> [!NOTE]
> This is a standalone implementation using NumPy. No external ADMM libraries are required.

## Proposed Changes

### New Files

#### [NEW] [admm_lasso.py](file:///home/sriv/admm_lasso.py)
- **Class `ADMMLasso`**:
    - `__init__(self, rho=1.0, max_iter=1000, tol=1e-4, lambda_reg=1.0)`
    - `fit(self, A, b)`: Solves the Lasso problem.
        - Precomputes Cholesky decomposition of $(A^T A + \rho I)$ for fast $x$-updates.
        - Iterates through $x$, $z$, and $u$ updates.
        - Checks for convergence (primal and dual residuals).
    - `_soft_threshold(self, v, kappa)`: Helper for the soft thresholding operator.

#### [NEW] [demo.py](file:///home/sriv/demo.py)
- Generates synthetic data (sparse signal).
- Runs `ADMMLasso`.
- Prints objective value and convergence status.
- Optionally compares with `sklearn.linear_model.Lasso` if installed (handled gracefully if not).

## Verification Plan

### Automated Tests
- Run `python3 demo.py` and check for:
    - Convergence (residuals decreasing).
    - Recovered support of the sparse signal (roughly matching the ground truth).
